{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_PATH = './data/labeled-comments.csv'\n",
    "TF_QUANTITY = 100\n",
    "GENERATE_TRAIN_TEST = False\n",
    "MODELS_PATH = './data/models'\n",
    "\n",
    "def get_vocabulary(df):\n",
    "    count_vectorizer = CountVectorizer(lowercase=False, stop_words=[])\n",
    "    cv_fit = count_vectorizer.fit_transform(df['content'])\n",
    "    word_list = count_vectorizer.get_feature_names()\n",
    "    frequecy_array = cv_fit.toarray()\n",
    "    count_list = frequecy_array.sum(axis=0)\n",
    "    vocabulary = (dict(zip(word_list, count_list)))\n",
    "    return vocabulary, frequecy_array, word_list\n",
    "\n",
    "\n",
    "def get_bigrams(df):\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        lowercase=False, stop_words=[], ngram_range=(2, 2))\n",
    "    cv_fit = count_vectorizer.fit_transform(df['content'])\n",
    "    word_list = count_vectorizer.get_feature_names()\n",
    "    frequecy_array = cv_fit.toarray()\n",
    "    count_list = frequecy_array.sum(axis=0)\n",
    "    vocabulary = (dict(zip(word_list, count_list)))\n",
    "    return vocabulary, frequecy_array, word_list\n",
    "\n",
    "\n",
    "def get_doc(df, chosen_words):\n",
    "    return df['content'].apply(lambda y: ' '.join(\n",
    "        [x for x in y.split() if x in chosen_words]))\n",
    "\n",
    "\n",
    "def get_bigram_doc(df, chosen_words):\n",
    "    def select_only_relevant_bigrams(text):\n",
    "        bigrams_in_text = [b for l in [text]\n",
    "                           for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
    "        return ' '.join([' '.join(w) for w in bigrams_in_text if ' '.join(w) in chosen_words])\n",
    "    return df['content'].apply(select_only_relevant_bigrams)\n",
    "\n",
    "\n",
    "def get_relevant_words(df):\n",
    "    return list(df.sort_values(\n",
    "        by=['diff'], ascending=False)['term'])\n",
    "\n",
    "def select_shared_terms(voc1, voc2):\n",
    "    # select words that are in bolth vocabularies\n",
    "    list_sexist_sorted_terms = []\n",
    "    for key, value in sorted(voc1.items(), key=lambda item: item[1]):\n",
    "        list_sexist_sorted_terms.append(key)\n",
    "\n",
    "    shared = []\n",
    "\n",
    "    for word in list_sexist_sorted_terms:\n",
    "        if word in voc2.keys():\n",
    "            shared.append(word) \n",
    "    return shared\n",
    "\n",
    "def select_word_freq(shared, sexist, not_sexist):\n",
    "    freq = {\n",
    "        'term': [],\n",
    "        'sexist-freq': [],\n",
    "        'not-sexist-freq': [],\n",
    "        'diff': []\n",
    "    }\n",
    "    for term in shared:\n",
    "        freq['term'].append(term)\n",
    "        if term in sexist.keys():\n",
    "            freq['sexist-freq'].append(sexist[term])\n",
    "        else:\n",
    "            freq['sexist-freq'].append(0)\n",
    "\n",
    "        if term in not_sexist.keys():\n",
    "            freq['not-sexist-freq'].append(not_sexist[term])\n",
    "        else:\n",
    "            freq['not-sexist-freq'].append(0)\n",
    "\n",
    "    freq['diff'] = sexist[term] - not_sexist[term]\n",
    "    freq = pd.DataFrame(freq)\n",
    "\n",
    "    # normalizind frequencies\n",
    "    sum_sexist = sum(freq['sexist-freq'])\n",
    "    freq['sexist-freq'] = freq['sexist-freq'].apply(\n",
    "        lambda x: x/sum_sexist)\n",
    "    sum_not_sexist = sum(freq['not-sexist-freq'])\n",
    "    freq['not-sexist-freq'] = freq['not-sexist-freq'].apply(\n",
    "        lambda x: x/sum_not_sexist)\n",
    "    freq['diff'] = freq['sexist-freq'] - \\\n",
    "        freq['not-sexist-freq']\n",
    "\n",
    "    sexist_terms = freq[freq['diff'] > 0]\n",
    "    not_sexist_terms = freq[freq['diff'] < 0]\n",
    "\n",
    "    # most relevant terms to sexist comments\n",
    "    sexist_ = sexist_terms.sort_values(\n",
    "        by='diff', ascending=False)\n",
    "\n",
    "    # most relevant terms to not sexist comments\n",
    "    not_sexist_ = not_sexist_terms.sort_values(\n",
    "        by='diff', ascending=True)\n",
    "\n",
    "    return sexist_, not_sexist_\n",
    "\n",
    "def generate_cross_validation_train_and_test(df):\n",
    "    train_size = math.floor(df.shape[0] * 0.9)\n",
    "    test_size = math.ceil(df.shape[0] * 0.1)\n",
    "    for i in range (0,10):\n",
    "        dataframe = df.sample(frac=1)\n",
    "        train = dataframe.iloc[:train_size]\n",
    "        test = dataframe.iloc[train_size:]\n",
    "\n",
    "        # ------ select most relevant unigrams and bigrams to sexist and not sexist context\n",
    "        sexist_comments = dataframe[dataframe['avg'] > 0.5]\n",
    "        sexist_vocabulary, sexist_frequency_array, sexist_word_list = get_vocabulary(sexist_comments)\n",
    "        sexist_bigrams, sexist_frequency_array, sexist_word_list = get_bigrams(sexist_comments)\n",
    "\n",
    "        not_sexist_comments = dataframe[dataframe['avg'] < 0.5]    \n",
    "        not_sexist_vocabulary, not_sexist_frequency_array, not_sexist_word_list = get_vocabulary(not_sexist_comments)\n",
    "        not_sexist_bigrams, not_sexist_frequency_array, not_sexist_word_list = get_bigrams(not_sexist_comments)\n",
    "\n",
    "        shared_unigrams = select_shared_terms(sexist_vocabulary, not_sexist_vocabulary)\n",
    "        sexist_unigrams, not_sexist_unigrams = select_word_freq(shared_unigrams, sexist_vocabulary, not_sexist_vocabulary)\n",
    "\n",
    "        shared_bigrams = select_shared_terms(sexist_bigrams, not_sexist_bigrams)\n",
    "        sexist_bigrams, not_sexist_bigrams = select_word_freq(shared_bigrams, sexist_bigrams, not_sexist_bigrams)\n",
    "\n",
    "        # ------ calculate and serialize term frequency to sexist and not sexist unigrams and bigrams\n",
    "        sexist_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            norm=None,\n",
    "            decode_error='replace',\n",
    "            max_features=TF_QUANTITY,\n",
    "        )\n",
    "        not_sexist_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            decode_error='replace',\n",
    "            max_features=TF_QUANTITY,\n",
    "        )\n",
    "        sexist_bigrams_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            ngram_range=(2, 2),\n",
    "            decode_error='replace',\n",
    "            max_features=TF_QUANTITY,\n",
    "        )\n",
    "        not_sexist_bigrams_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            ngram_range=(2, 2),\n",
    "            decode_error='replace',\n",
    "            max_features=TF_QUANTITY,\n",
    "        )\n",
    "\n",
    "        relevant_sexist_words = get_relevant_words(sexist_unigrams)\n",
    "        relevant_not_sexist_words = get_relevant_words(\n",
    "            not_sexist_unigrams)\n",
    "\n",
    "        relevant_sexist_bigrams = get_relevant_words(sexist_bigrams)\n",
    "        relevant_not_sexist_bigrams = get_relevant_words(\n",
    "            not_sexist_bigrams)\n",
    "\n",
    "        # ------ sexist unigrams TF\n",
    "        sexist_doc = get_doc(sexist_comments, relevant_sexist_words)\n",
    "        not_sexist_doc = get_doc(not_sexist_comments, relevant_sexist_words)\n",
    "        sexist_tf = pd.DataFrame(sexist_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_tf = pd.DataFrame(not_sexist_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        tf_sexist_dataframe = pd.concat([sexist_tf, not_sexist_tf]).fillna(0)\n",
    "        tf_sexist_dataframe.columns = [f'TFus_{i}' for i in range(100)]\n",
    "\n",
    "        # ------ sexist bigrams TF\n",
    "        sexist_doc = get_bigram_doc(sexist_comments, relevant_sexist_bigrams)\n",
    "        not_sexist_doc = get_bigram_doc(not_sexist_comments, relevant_sexist_bigrams)\n",
    "        sexist_bigrams_tf = pd.DataFrame(sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_bigrams_tf = pd.DataFrame(not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        tf_sexist_bigrams_dataframe = pd.concat([sexist_bigrams_tf, not_sexist_bigrams_tf]).fillna(0)\n",
    "        tf_sexist_bigrams_dataframe.columns = [f'TFbs_{i}' for i in range(100)]\n",
    "\n",
    "        # ------ not sexist unigrams TF\n",
    "        sexist_doc = get_doc(sexist_comments, relevant_not_sexist_words)\n",
    "        not_sexist_doc = get_doc(not_sexist_comments, relevant_not_sexist_words)\n",
    "        sexist_tf = pd.DataFrame(sexist_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_tf = pd.DataFrame(not_sexist_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        tf_not_sexist_dataframe = pd.concat([sexist_tf, not_sexist_tf]).fillna(0)\n",
    "        tf_not_sexist_dataframe.columns = [f'TFun_{i}' for i in range(100)]\n",
    "\n",
    "        # ------ not sexist bigrams TF\n",
    "        sexist_doc = get_bigram_doc(sexist_comments, relevant_not_sexist_bigrams)\n",
    "        not_sexist_doc = get_bigram_doc(not_sexist_comments, relevant_not_sexist_bigrams)\n",
    "        sexist_bigrams_tf = pd.DataFrame(sexist_bigrams_vectorizer.fit_transform(sexist_doc).toarray())\n",
    "        not_sexist_bigrams_tf = pd.DataFrame(not_sexist_bigrams_vectorizer.fit_transform(not_sexist_doc).toarray())\n",
    "        tf_not_sexist_bigrams_dataframe = pd.concat([sexist_bigrams_tf, not_sexist_bigrams_tf]).fillna(0)\n",
    "        tf_not_sexist_bigrams_dataframe.columns = [f'TFbn_{i}' for i in range(100)]\n",
    "\n",
    "        tf_dataframe = pd.concat([tf_sexist_dataframe,\n",
    "                                  tf_not_sexist_dataframe,\n",
    "                                  tf_sexist_bigrams_dataframe,\n",
    "                                  tf_not_sexist_bigrams_dataframe], axis=1)\n",
    "\n",
    "        # ------ define quantitative features to train\n",
    "        likes_df = np.array(pd.concat([sexist_comments['likes'], not_sexist_comments['likes']]).fillna(0))\n",
    "        dislikes_df = np.array(pd.concat([sexist_comments['dislikes'], not_sexist_comments['dislikes']]).fillna(0))\n",
    "        char_qty_df = np.array(pd.concat([sexist_comments['char-qty'], not_sexist_comments['char-qty']]).fillna(0))\n",
    "        word_qty_df = np.array(pd.concat([sexist_comments['word-qty'], not_sexist_comments['word-qty']]).fillna(0))\n",
    "        sexist_y = sexist_comments['avg'].apply(lambda x: 1)\n",
    "        not_sexist_y = not_sexist_comments['avg'].apply(lambda x: 0)\n",
    "        y_df = np.array(pd.concat([sexist_y, not_sexist_y]))\n",
    "\n",
    "        X_train = tf_dataframe\n",
    "        X_train['likes'] = likes_df\n",
    "        X_train['dislikes'] = dislikes_df\n",
    "        X_train['char-qty'] = char_qty_df\n",
    "        X_train['word-qty'] = word_qty_df\n",
    "        X_train['sexist'] = y_df\n",
    "        X_train = X_train.fillna(0)\n",
    "        X_train = X_train.sample(frac=1)\n",
    "        X_train.to_csv(f'./data/{i+1}_train.csv', index=False)\n",
    "\n",
    "        # define features to test\n",
    "\n",
    "        doc_vectorizer = TfidfVectorizer(\n",
    "            stop_words=[],\n",
    "            use_idf=False,\n",
    "            norm=None,\n",
    "            decode_error='replace',\n",
    "            max_features=TF_QUANTITY,\n",
    "        )    \n",
    "\n",
    "        doc = get_doc(test, relevant_sexist_words)\n",
    "        tf = pd.DataFrame(doc_vectorizer.fit_transform(doc).toarray(), columns=[f'TFus_{i}' for i in range(100)])\n",
    "        tf_sexist_dataframe = pd.concat([tf]).fillna(0)\n",
    "\n",
    "        doc = get_bigram_doc(test, relevant_sexist_bigrams)\n",
    "        tf = pd.DataFrame(doc_vectorizer.fit_transform(doc).toarray(), columns=[f'TFbs_{i}' for i in range(100)])\n",
    "        tf_sexist_bigrams_dataframe = pd.concat([tf]).fillna(0)\n",
    "\n",
    "        doc = get_doc(test, relevant_not_sexist_words)\n",
    "        tf = pd.DataFrame(doc_vectorizer.fit_transform(doc).toarray(), columns=[f'TFun_{i}' for i in range(100)])\n",
    "        tf_not_sexist_dataframe = pd.concat([tf]).fillna(0)\n",
    "\n",
    "        doc = get_bigram_doc(test, relevant_not_sexist_bigrams)\n",
    "        tf = pd.DataFrame(doc_vectorizer.fit_transform(doc).toarray(), columns=[f'TFbn_{i}' for i in range(100)])\n",
    "        tf_not_sexist_bigrams_dataframe = pd.concat([tf]).fillna(0)\n",
    "\n",
    "        tf_test_dataframe = pd.concat([tf_sexist_dataframe,\n",
    "                                      tf_not_sexist_dataframe,\n",
    "                                      tf_sexist_bigrams_dataframe,\n",
    "                                      tf_not_sexist_bigrams_dataframe], axis=1)\n",
    "        X_test = tf_dataframe\n",
    "        X_test['likes'] = likes_df\n",
    "        X_test['dislikes'] = dislikes_df\n",
    "        X_test['char-qty'] = char_qty_df\n",
    "        X_test['word-qty'] = word_qty_df\n",
    "        X_test['sexist'] = y_df\n",
    "        X_test = X_test.fillna(0)\n",
    "        X_test = X_test.sample(frac=1)\n",
    "        X_test.to_csv(f'./data/{i+1}_test.csv', index=False)\n",
    "\n",
    "def select_df_columns(df, columns):\n",
    "    list_features = [df[df.columns[list(df.columns).index(x)]] for x in columns]\n",
    "    X = pd.DataFrame(list_features).transpose()\n",
    "    return X\n",
    "\n",
    "def build_results_report(title,precision,recall,fscore):\n",
    "    report = {\n",
    "        'title': title,\n",
    "        'precision': {'1': [], '0': []},\n",
    "        'recall': {'1': [], '0': []},\n",
    "        'f1': {'1': [], '0': []}}\n",
    "    report['precision']['1'].append(precision[0])\n",
    "    report['recall']['1'].append(recall[0])\n",
    "    report['f1']['1'].append(fscore[0])\n",
    "    report['precision']['0'].append(precision[1])\n",
    "    report['recall']['0'].append(recall[1])\n",
    "    report['f1']['0'].append(fscore[1])\n",
    "    return report\n",
    "\n",
    "def print_report(report):\n",
    "    print(f'>>>> {report[\"title\"]} results')\n",
    "    print('\\t\\t sexist \\t not-sexist')\n",
    "    print('precision\\t %.5f \\t %.5f' % (\n",
    "        np.mean(report[\"precision\"][\"1\"]), np.mean(report[\"precision\"][\"0\"])))\n",
    "    print('recall\\t\\t %.5f \\t %.5f' %\n",
    "          (np.mean(report[\"recall\"][\"1\"]), np.mean(report[\"recall\"][\"0\"])))\n",
    "    print('f1\\t\\t %.5f \\t %.5f' %\n",
    "          (np.mean(report[\"f1\"][\"1\"]), np.mean(report[\"f1\"][\"0\"])))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df = pd.read_csv(DATAFRAME_PATH)\n",
    "\n",
    "if GENERATE_TRAIN_TEST:\n",
    "    generate_cross_validation_train_and_test(df)\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(0, 1)):\n",
    "    # ------- loading data\n",
    "    train = pd.read_csv(f'./data/{i+1}_train.csv')\n",
    "    test = pd.read_csv(f'./data/{i+1}_test.csv')\n",
    "    X_train = train.drop(columns=['sexist'])\n",
    "    y_train = train['sexist']\n",
    "    X_test = test.drop(columns=['sexist'])\n",
    "    y_test = test['sexist']\n",
    "    \n",
    "    # ------- selection features combination\n",
    "    feature_combinations = {\n",
    "        'TFus' : [x for x in list(train.columns) if 'TFus' in x], # sexist unigrams TFs\n",
    "        'TFun' : [x for x in list(train.columns) if 'TFun' in x], # not-sexist unigrams TFs\n",
    "        'TFbs' : [x for x in list(train.columns) if 'TFbs' in x], # sexist bigrams TFs\n",
    "        'TFbn' : [x for x in list(train.columns) if 'TFbn' in x], # not sexist bigrams TFs\n",
    "        'L' : ['likes'], # likes quantity\n",
    "        'D' : ['dislikes'], # dislikes quantity\n",
    "        'CQ' : ['char-qty'], # chars quantity\n",
    "        'WQ' : ['word-qty'] # word quantity\n",
    "    }\n",
    "    feature_combinations['LD'] = feature_combinations['L'] + feature_combinations['D']\n",
    "    feature_combinations['TFu'] = feature_combinations['TFus'] + feature_combinations['TFun']\n",
    "    feature_combinations['TFb'] = feature_combinations['TFbs'] + feature_combinations['TFbn']\n",
    "    feature_combinations['TF'] = feature_combinations['TFu'] + feature_combinations['TFb']\n",
    "    feature_combinations['CQWQ'] = feature_combinations['CQ'] + feature_combinations['WQ']\n",
    "    feature_combinations['Q'] = feature_combinations['LD'] + feature_combinations['CQWQ']\n",
    "    feature_combinations['TFuQ'] = feature_combinations['TFu'] + feature_combinations['Q']    \n",
    "    \n",
    "    combination_results = dict({})\n",
    "\n",
    "    for combination in tqdm(feature_combinations.keys()):\n",
    "        X_train_ = select_df_columns(X_train, feature_combinations[combination])\n",
    "        X_test_ = select_df_columns(X_test, feature_combinations[combination])\n",
    "\n",
    "        # ------ training SVM\n",
    "        model_path = f'{MODELS_PATH}/{i}_SVM_{combination}'\n",
    "        try:\n",
    "            svm = pickle.load(open(model_path, 'rb'))\n",
    "        except:\n",
    "            svm = SVC(gamma=1, C=10, kernel='linear')\n",
    "            svm.fit(X_train_ ,y_train)\n",
    "            pickle.dump(svm, open(model_path, 'wb'))\n",
    "        \n",
    "        # ------ testing SVM\n",
    "        y = svm.predict(X_test_)\n",
    "        precision, recall, fscore, _ = score(y_test, y, average=None, labels=[1, 0])\n",
    "        report = build_results_report(f'{i}_SVM_{combination}', precision, recall, fscore)\n",
    "        combination_results[f'{i}_SVM_{combination}'] = report\n",
    "        # print_report(report)\n",
    "\n",
    "        # ------ training KNN\n",
    "        model_path = f'{MODELS_PATH}/{i}_KNN_{combination}'\n",
    "        try:\n",
    "            knn = pickle.load(open(model_path, 'rb'))\n",
    "        except:\n",
    "            knn = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='euclidean')\n",
    "            knn.fit(X_train_,y_train)\n",
    "            pickle.dump(knn, open(model_path, 'wb'))\n",
    "\n",
    "        # ------ testing KNN\n",
    "        y = knn.predict(X_test_)\n",
    "        precision, recall, fscore, _ = score(y_test, y, average=None, labels=[1, 0])\n",
    "        report = build_results_report(f'{i}_KNN_{combination}', precision, recall, fscore)\n",
    "        combination_results[f'{i}_KNN_{combination}'] = report\n",
    "        # print_report(report)\n",
    "    \n",
    "        # ------ training RFC\n",
    "        model_path = f'{MODELS_PATH}/{i}_RFC_{combination}'\n",
    "        try:\n",
    "            rfc = pickle.load(open(model_path, 'rb'))\n",
    "        except:\n",
    "            rfc = RandomForestClassifier(n_estimators=200, max_depth=8)\n",
    "            rfc.fit(X_train_,y_train)\n",
    "            pickle.dump(rfc, open(model_path, 'wb'))\n",
    "\n",
    "        # ------ testing RFC\n",
    "        y = rfc.predict(X_test_)\n",
    "        precision, recall, fscore, _ = score(y_test, y, average=None, labels=[1, 0])\n",
    "        report = build_results_report(f'{i}_RFC_{combination}', precision, recall, fscore)\n",
    "        combination_results[f'{i}_RFC_{combination}'] = report\n",
    "        # print_report(report)\n",
    "\n",
    "    results.append(combination_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    'title': 'Teste',\n",
    "    'precision': {'1': [], '0': []},\n",
    "    'recall': {'1': [], '0': []},\n",
    "    'f1': {'1': [], '0': []},\n",
    "    'confusion_matrix': None}\n",
    "report['precision']['1'].append(precision[0])\n",
    "report['recall']['1'].append(recall[0])\n",
    "report['f1']['1'].append(fscore[0])\n",
    "report['precision']['0'].append(precision[1])\n",
    "report['recall']['0'].append(recall[1])\n",
    "report['f1']['0'].append(fscore[1])\n",
    "c_matrix = confusion_matrix(y_test, y,  labels=[1, 0])\n",
    "report['confusion_matrix'] = pd.DataFrame(\n",
    "    c_matrix, columns=['T', 'F'], index=['T', 'F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFus = [x for x in list(X_train.columns) if 'TFus' in x]\n",
    "TFun = [x for x in list(X_train.columns) if 'TFun' in x]\n",
    "TFbs = [x for x in list(X_train.columns) if 'TFbs' in x]\n",
    "TFbu = [x for x in list(X_train.columns) if 'TFbu' in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
