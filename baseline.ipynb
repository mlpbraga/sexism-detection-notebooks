{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: vaderSentiment in /opt/homebrew/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: textstat in /opt/homebrew/lib/python3.9/site-packages (0.7.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.9/site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: pyphen in /opt/homebrew/lib/python3.9/site-packages (from textstat) (0.12.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/homebrew/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/homebrew/lib/python3.9/site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/homebrew/lib/python3.9/site-packages (from seaborn) (1.22.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/homebrew/lib/python3.9/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/homebrew/lib/python3.9/site-packages (from seaborn) (1.7.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (4.28.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (9.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/homebrew/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (4.28.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (1.22.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (9.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install vaderSentiment textstat\n",
    "!pip3 install seaborn\n",
    "!pip3 install matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>content</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>votes</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>label</th>\n",
       "      <th>char-qty</th>\n",
       "      <th>word-qty</th>\n",
       "      <th>legibility-index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1489</td>\n",
       "      <td>MUITO MAIS LEGAL  RRSRSRRSRSRS</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>223.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>273</td>\n",
       "      <td>Canh√£o de guerra.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>190.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2574</td>\n",
       "      <td>femi o que?</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>217.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>951</td>\n",
       "      <td>Concordo plenamente Jaqueline!! Outro dia ouvi...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>27</td>\n",
       "      <td>228.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2520</td>\n",
       "      <td>Feminista √© uma mulher encalhada que precisa d...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>226.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>55</td>\n",
       "      <td>Brigue esquisitinha</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>204.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>3858</td>\n",
       "      <td>Pois √©, todo o bozopata √© assim! Depois que pe...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>35</td>\n",
       "      <td>228.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>3984</td>\n",
       "      <td>Ser√° que ningu√©m tem coragem de enfrentar algu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>60</td>\n",
       "      <td>230.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>790</td>\n",
       "      <td>Perfeito!</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>163.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>2850</td>\n",
       "      <td>Impress√£o sua, a diferen√ßa √© que agora voc√™ v√™...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "      <td>44</td>\n",
       "      <td>229.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3533 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                            content  likes  \\\n",
       "0           1489                     MUITO MAIS LEGAL  RRSRSRRSRSRS    2.0   \n",
       "1            273                                 Canh√£o de guerra.     2.0   \n",
       "2           2574                                        femi o que?   10.0   \n",
       "3            951  Concordo plenamente Jaqueline!! Outro dia ouvi...   20.0   \n",
       "4           2520  Feminista √© uma mulher encalhada que precisa d...   11.0   \n",
       "...          ...                                                ...    ...   \n",
       "3583          55                                Brigue esquisitinha    0.0   \n",
       "3584        3858  Pois √©, todo o bozopata √© assim! Depois que pe...    3.0   \n",
       "3585        3984  Ser√° que ningu√©m tem coragem de enfrentar algu...    0.0   \n",
       "3586         790                                          Perfeito!    2.0   \n",
       "3587        2850  Impress√£o sua, a diferen√ßa √© que agora voc√™ v√™...    4.0   \n",
       "\n",
       "      dislikes  votes       avg       std  label  char-qty  word-qty  \\\n",
       "0          0.0      4  0.000000  0.000000      0        30         5   \n",
       "1          4.0      3  1.000000  0.000000      1        18         4   \n",
       "2         11.0      3  1.000000  0.000000      1        11         3   \n",
       "3          0.0      3  0.666667  0.577350      1       161        27   \n",
       "4         11.0      4  1.000000  0.000000      1        66        11   \n",
       "...        ...    ...       ...       ...    ...       ...       ...   \n",
       "3583       1.0      3  0.666667  0.577350      1        19         2   \n",
       "3584       0.0      4  0.250000  0.500000      0       192        35   \n",
       "3585       0.0      4  0.000000  0.000000      0       368        60   \n",
       "3586       2.0      5  0.400000  0.547723      0         9         1   \n",
       "3587       0.0      3  0.333333  0.577350      0       270        44   \n",
       "\n",
       "      legibility-index  \n",
       "0               223.62  \n",
       "1               190.91  \n",
       "2               217.59  \n",
       "3               228.87  \n",
       "4               226.84  \n",
       "...                ...  \n",
       "3583            204.50  \n",
       "3584            228.41  \n",
       "3585            230.27  \n",
       "3586            163.22  \n",
       "3587            229.00  \n",
       "\n",
       "[3533 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_comments = pd.read_csv('data/labeled_comments.csv', sep=';')\n",
    "labeled_comments = labeled_comments[labeled_comments['label'] != -1]\n",
    "labeled_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Dataset fields description</b>\n",
    "    <hline/>\n",
    "    <p><b>comment_id</b>: unique identifier to each comment from database</p>\n",
    "    <p><b>content</b>: comment text content</p>\n",
    "    <p><b>likes</b>: comment likes quantity</p>\n",
    "    <p><b>dislikes</b>: comment dislikes quantity</p>\n",
    "    <p><b>votes</b>: number of users that labeled the comment</p>\n",
    "    <p><b>avg</b>: average of each vote value to the comment</p>\n",
    "    <p><b>std</b>: standard deviation of each vote value to the comment</p>\n",
    "    <p><b>label</b>: final label assigned to the comment, label 1 represents sexist comments and label 0 represets not sexist comments</p>\n",
    "    <p><b>char-qty</b>: number of characters in the comment </p>\n",
    "    <p><b>word-qty</b>: number of words in the comment</p>\n",
    "    <p><b>legibility-index</b>: value of FleshBr legibility index to this comment</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>votes</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>label</th>\n",
       "      <th>char-qty</th>\n",
       "      <th>word-qty</th>\n",
       "      <th>legibility-index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3533.000000</td>\n",
       "      <td>3287.000000</td>\n",
       "      <td>3287.000000</td>\n",
       "      <td>3533.000000</td>\n",
       "      <td>3533.000000</td>\n",
       "      <td>3533.000000</td>\n",
       "      <td>3533.000000</td>\n",
       "      <td>3533.000000</td>\n",
       "      <td>3533.000000</td>\n",
       "      <td>3533.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1934.606850</td>\n",
       "      <td>15.720110</td>\n",
       "      <td>9.899605</td>\n",
       "      <td>3.350977</td>\n",
       "      <td>0.525278</td>\n",
       "      <td>0.271735</td>\n",
       "      <td>0.515426</td>\n",
       "      <td>138.527031</td>\n",
       "      <td>24.275403</td>\n",
       "      <td>223.161336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1141.239782</td>\n",
       "      <td>40.303846</td>\n",
       "      <td>42.000177</td>\n",
       "      <td>0.652721</td>\n",
       "      <td>0.379685</td>\n",
       "      <td>0.278408</td>\n",
       "      <td>0.499833</td>\n",
       "      <td>136.155936</td>\n",
       "      <td>23.547024</td>\n",
       "      <td>15.694895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>993.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>223.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1899.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>228.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2794.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>229.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4283.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>230.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        comment_id        likes     dislikes        votes          avg  \\\n",
       "count  3533.000000  3287.000000  3287.000000  3533.000000  3533.000000   \n",
       "mean   1934.606850    15.720110     9.899605     3.350977     0.525278   \n",
       "std    1141.239782    40.303846    42.000177     0.652721     0.379685   \n",
       "min       4.000000     0.000000     0.000000     2.000000     0.000000   \n",
       "25%     993.000000     2.000000     0.000000     3.000000     0.250000   \n",
       "50%    1899.000000     5.000000     1.000000     3.000000     0.600000   \n",
       "75%    2794.000000    14.000000     6.000000     4.000000     1.000000   \n",
       "max    4283.000000   729.000000  1196.000000     9.000000     1.000000   \n",
       "\n",
       "               std        label     char-qty     word-qty  legibility-index  \n",
       "count  3533.000000  3533.000000  3533.000000  3533.000000       3533.000000  \n",
       "mean      0.271735     0.515426   138.527031    24.275403        223.161336  \n",
       "std       0.278408     0.499833   136.155936    23.547024         15.694895  \n",
       "min       0.000000     0.000000     2.000000     1.000000         -5.300000  \n",
       "25%       0.000000     0.000000    47.000000     9.000000        223.620000  \n",
       "50%       0.000000     1.000000    94.000000    17.000000        228.290000  \n",
       "75%       0.577350     1.000000   179.000000    32.000000        229.810000  \n",
       "max       0.577350     1.000000   958.000000   176.000000        230.300000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_comments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labeled_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    # tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tweet = ' '.join(tweet.lower().strip().split(' '))\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    # tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    tweet = ' '.join(tweet.lower().strip().split(' '))\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aquel', 'depoi', 'entr', 'ess', 'est', 'estamo', 'estejamo', 'estev', 'estiv', 'estivemo', 'estivermo', 'estivess', 'estiv√©ramo', 'estiv√©ssemo', 'est√°vamo', 'fomo', 'formo', 'foss', 'f√¥ramo', 'f√¥ssemo', 'hajamo', 'havemo', 'houv', 'houvemo', 'houveremo', 'houvermo', 'houver√≠amo', 'houvess', 'houv√©ramo', 'houv√©ssemo', 'ma', 'mai', 'n√≥', 'sejamo', 'seremo', 'ser√≠amo', 'somo', 'temo', 'tenhamo', 'teremo', 'ter√≠amo', 'tivemo', 'tivermo', 'tivess', 'tiv√©ramo', 'tiv√©ssemo', 't√≠nhamo', 'vo', '√©ramo'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(comments).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/luisabraga/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "#Get POS tags for comments and save as a string\n",
    "comment_tags = []\n",
    "for t in comments:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    comment_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(comment_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 0 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.60      0.59       335\n",
      "           1       0.62      0.59      0.61       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.60       707\n",
      "weighted avg       0.60      0.60      0.60       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 1 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.61      0.59       335\n",
      "           1       0.62      0.58      0.60       372\n",
      "\n",
      "    accuracy                           0.59       707\n",
      "   macro avg       0.59      0.59      0.59       707\n",
      "weighted avg       0.60      0.59      0.59       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 2 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59       335\n",
      "           1       0.63      0.60      0.61       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.60       707\n",
      "weighted avg       0.60      0.60      0.60       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 3 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.67      0.61       335\n",
      "           1       0.64      0.53      0.58       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.59       707\n",
      "weighted avg       0.60      0.60      0.59       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 4 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59       335\n",
      "           1       0.63      0.60      0.61       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.60       707\n",
      "weighted avg       0.60      0.60      0.60       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 5 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.62      0.60       335\n",
      "           1       0.63      0.58      0.60       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.60       707\n",
      "weighted avg       0.60      0.60      0.60       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 6 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.60      0.58       335\n",
      "           1       0.62      0.59      0.61       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.60       707\n",
      "weighted avg       0.60      0.60      0.60       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 7 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.66      0.61       335\n",
      "           1       0.64      0.54      0.59       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.60       707\n",
      "weighted avg       0.60      0.60      0.60       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 8 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.65      0.60       335\n",
      "           1       0.63      0.55      0.59       372\n",
      "\n",
      "    accuracy                           0.59       707\n",
      "   macro avg       0.60      0.60      0.59       707\n",
      "weighted avg       0.60      0.59      0.59       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 9 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59       335\n",
      "           1       0.63      0.60      0.61       372\n",
      "\n",
      "    accuracy                           0.60       707\n",
      "   macro avg       0.60      0.60      0.60       707\n",
      "weighted avg       0.60      0.60      0.60       707\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63       335\n",
      "           1       0.66      0.60      0.63       372\n",
      "\n",
      "    accuracy                           0.63       707\n",
      "   macro avg       0.63      0.63      0.63       707\n",
      "weighted avg       0.63      0.63      0.63       707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "X = pd.DataFrame(M)\n",
    "y = df['label'].astype(int)\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model_svc = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_train, y_train)\n",
    "    model_lr = LogisticRegression(class_weight='balanced',penalty='l2',C=0.01).fit(X_train, y_train)\n",
    "\n",
    "    y_preds_svc = model_svc.predict(X_test)\n",
    "    y_preds_lr = model_lr.predict(X_test)\n",
    "    print(f'------------------- {i} --------------------')\n",
    "    print('SVM')\n",
    "    print(classification_report(y_test,y_preds_svc))\n",
    "    print('LR')\n",
    "    print(classification_report(y_test,y_preds_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/baseline_results.txt') as fp:\n",
    "    file_lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = {\n",
    "    'precision_0': [],\n",
    "    'recall_0': [],\n",
    "    'f1_0': [],\n",
    "    'precision_1': [],\n",
    "    'recall_1': [],\n",
    "    'f1_1': [],\n",
    "    'f1_macro': []\n",
    "}\n",
    "\n",
    "lr = {\n",
    "    'precision_0': [],\n",
    "    'recall_0': [],\n",
    "    'f1_0': [],\n",
    "    'precision_1': [],\n",
    "    'recall_1': [],\n",
    "    'f1_1': [],\n",
    "    'f1_macro': []\n",
    "}\n",
    "\n",
    "data = []\n",
    "for line in file_lines:\n",
    "    is_data_line = ('----' not in line) and\\\n",
    "                   ('SVM' not in line) and\\\n",
    "                   ('LR' not in line) and\\\n",
    "                   ('precision' not in line) and\\\n",
    "                   ('\\n' != line) and \\\n",
    "                   ('accuracy' not in line) and\\\n",
    "                   ('weightedavg' not in line)\n",
    "    if is_data_line:\n",
    "        data.append(line.replace('           ','')\\\n",
    "                        .replace('      ', ';')\\\n",
    "                        .replace(' ', '')\\\n",
    "                        .replace('\\n', '')\\\n",
    "                        .split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data),8):\n",
    "    row = data[i:i+8]\n",
    "    svm['precision_0'].append(float(row[0][1]))\n",
    "    svm['recall_0'].append(float(row[0][2]))\n",
    "    svm['f1_0'].append(float(row[0][3]))\n",
    "    svm['precision_1'].append(float(row[1][1]))\n",
    "    svm['recall_1'].append(float(row[1][2]))\n",
    "    svm['f1_1'].append(float(row[1][3]))\n",
    "    svm['f1_macro'].append(float(row[2][3]))\n",
    "    lr['f1_macro'].append(float(row[6][3]))\n",
    "    lr['precision_0'].append(float(row[4][1]))\n",
    "    lr['recall_0'].append(float(row[4][2]))\n",
    "    lr['f1_0'].append(float(row[4][3]))\n",
    "    lr['precision_1'].append(float(row[5][1]))\n",
    "    lr['recall_1'].append(float(row[5][2]))\n",
    "    lr['f1_1'].append(float(row[5][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.63}&{0.57}&{0.57}&{0.64}&{0.59}&{0.6}\n"
     ]
    }
   ],
   "source": [
    "ps = round(np.mean(svm['precision_1']),2)\n",
    "pn = round(np.mean(svm['precision_0']),2)\n",
    "rs = round(np.mean(svm['recall_1']),2)\n",
    "rn = round(np.mean(svm['recall_0']),2)\n",
    "fs = round(np.mean(svm['f1_1']),2)\n",
    "fn = round(np.mean(svm['f1_0']),2)\n",
    "print (f'{{{ps}}}&{{{pn}}}&{{{rs}}}&{{{rn}}}&{{{fs}}}&{{{fn}}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.66}&{0.6}&{0.6}&{0.66}&{0.63}&{0.63}\n"
     ]
    }
   ],
   "source": [
    "ps = round(np.mean(lr['precision_1']),2)\n",
    "pn = round(np.mean(lr['precision_0']),2)\n",
    "rs = round(np.mean(lr['recall_1']),2)\n",
    "rn = round(np.mean(lr['recall_0']),2)\n",
    "fs = round(np.mean(lr['f1_1']),2)\n",
    "fn = round(np.mean(lr['f1_0']),2)\n",
    "print (f'{{{ps}}}&{{{pn}}}&{{{rs}}}&{{{rn}}}&{{{fs}}}&{{{fn}}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
