{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in d:\\arquivos e programas\\anaconda\\lib\\site-packages (3.3.2)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.0-py3-none-any.whl (99 kB)\n",
      "Requirement already satisfied: requests in d:\\arquivos e programas\\anaconda\\lib\\site-packages (from vaderSentiment) (2.22.0)\n",
      "Collecting pyphen\n",
      "  Downloading Pyphen-0.10.0-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\arquivos e programas\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\arquivos e programas\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\arquivos e programas\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\arquivos e programas\\anaconda\\lib\\site-packages (from requests->vaderSentiment) (1.25.8)\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.10.0 textstat-0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'd:\\arquivos e programas\\anaconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install vaderSentiment textstat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>content</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>votes</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>label</th>\n",
       "      <th>char-qty</th>\n",
       "      <th>word-qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1489</td>\n",
       "      <td>MUITO MAIS LEGAL  RRSRSRRSRSRS</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>273</td>\n",
       "      <td>Canhão de guerra.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2574</td>\n",
       "      <td>femi o que?</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>951</td>\n",
       "      <td>Concordo plenamente Jaqueline!! Outro dia ouvi...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2520</td>\n",
       "      <td>Feminista é uma mulher encalhada que precisa d...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>55</td>\n",
       "      <td>Brigue esquisitinha</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>3858</td>\n",
       "      <td>Pois é, todo o bozopata é assim! Depois que pe...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>3984</td>\n",
       "      <td>Será que ninguém tem coragem de enfrentar algu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>790</td>\n",
       "      <td>Perfeito!</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3570</th>\n",
       "      <td>2850</td>\n",
       "      <td>Impressão sua, a diferença é que agora você vê...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3571 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                            content  likes  \\\n",
       "0           1489                     MUITO MAIS LEGAL  RRSRSRRSRSRS    2.0   \n",
       "1            273                                 Canhão de guerra.     2.0   \n",
       "2           2574                                        femi o que?   10.0   \n",
       "3            951  Concordo plenamente Jaqueline!! Outro dia ouvi...   20.0   \n",
       "4           2520  Feminista é uma mulher encalhada que precisa d...   11.0   \n",
       "...          ...                                                ...    ...   \n",
       "3566          55                                Brigue esquisitinha    0.0   \n",
       "3567        3858  Pois é, todo o bozopata é assim! Depois que pe...    3.0   \n",
       "3568        3984  Será que ninguém tem coragem de enfrentar algu...    0.0   \n",
       "3569         790                                          Perfeito!    2.0   \n",
       "3570        2850  Impressão sua, a diferença é que agora você vê...    4.0   \n",
       "\n",
       "      dislikes  votes       avg      std  label  char-qty  word-qty  \n",
       "0          0.0      2  0.000000  0.00000      0        30         5  \n",
       "1          4.0      2  1.000000  0.00000      1        18         4  \n",
       "2         11.0      2  1.000000  0.00000      1        11         3  \n",
       "3          0.0      3  0.666667  0.57735      1       161        27  \n",
       "4         11.0      3  1.000000  0.00000      1        66        11  \n",
       "...        ...    ...       ...      ...    ...       ...       ...  \n",
       "3566       1.0      3  0.666667  0.57735      1        19         2  \n",
       "3567       0.0      3  0.333333  0.57735      0       192        35  \n",
       "3568       0.0      1  0.000000      NaN      0       368        60  \n",
       "3569       2.0      3  0.333333  0.57735      0         9         1  \n",
       "3570       0.0      3  0.333333  0.57735      0       270        44  \n",
       "\n",
       "[3571 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_comments = pd.read_csv('data/labeled-comments.csv')\n",
    "labeled_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Dataset fields description</b>\n",
    "    <hline/>\n",
    "    <p><b>comment_id</b>: unique identifier to each comment from database</p>\n",
    "    <p><b>content</b>: comment text content</p>\n",
    "    <p><b>likes</b>: comment likes quantity</p>\n",
    "    <p><b>dislikes</b>: comment dislikes quantity</p>\n",
    "    <p><b>votes</b>: number of users that labeled the comment</p>\n",
    "    <p><b>avg</b>: average of each vote value to the comment</p>\n",
    "    <p><b>std</b>: standard deviation of each vote value to the comment</p>\n",
    "    <p><b>label</b>: final label assigned to the comment, label 1 represents sexist comments and label 0 represets not sexist comments</p>\n",
    "    <p><b>char-qty</b>: number of characters in the comment </p>\n",
    "    <p><b>word-qty</b>: number of words in the comment</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>votes</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>label</th>\n",
       "      <th>char-qty</th>\n",
       "      <th>word-qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3571.000000</td>\n",
       "      <td>3323.000000</td>\n",
       "      <td>3323.000000</td>\n",
       "      <td>3571.000000</td>\n",
       "      <td>3571.000000</td>\n",
       "      <td>2886.000000</td>\n",
       "      <td>3571.000000</td>\n",
       "      <td>3571.000000</td>\n",
       "      <td>3571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1923.082610</td>\n",
       "      <td>15.611496</td>\n",
       "      <td>9.856455</td>\n",
       "      <td>2.534304</td>\n",
       "      <td>0.523321</td>\n",
       "      <td>0.230954</td>\n",
       "      <td>0.523663</td>\n",
       "      <td>140.439373</td>\n",
       "      <td>24.494539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1135.048496</td>\n",
       "      <td>40.102726</td>\n",
       "      <td>41.783877</td>\n",
       "      <td>1.032327</td>\n",
       "      <td>0.420923</td>\n",
       "      <td>0.277073</td>\n",
       "      <td>0.499510</td>\n",
       "      <td>178.984760</td>\n",
       "      <td>27.111817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>988.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1886.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2783.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4283.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7050.000000</td>\n",
       "      <td>819.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        comment_id        likes     dislikes        votes          avg  \\\n",
       "count  3571.000000  3323.000000  3323.000000  3571.000000  3571.000000   \n",
       "mean   1923.082610    15.611496     9.856455     2.534304     0.523321   \n",
       "std    1135.048496    40.102726    41.783877     1.032327     0.420923   \n",
       "min       4.000000     0.000000     0.000000     1.000000     0.000000   \n",
       "25%     988.500000     2.000000     0.000000     2.000000     0.000000   \n",
       "50%    1886.000000     5.000000     1.000000     3.000000     0.666667   \n",
       "75%    2783.500000    14.000000     6.000000     3.000000     1.000000   \n",
       "max    4283.000000   729.000000  1196.000000     7.000000     1.000000   \n",
       "\n",
       "               std        label     char-qty     word-qty  \n",
       "count  2886.000000  3571.000000  3571.000000  3571.000000  \n",
       "mean      0.230954     0.523663   140.439373    24.494539  \n",
       "std       0.277073     0.499510   178.984760    27.111817  \n",
       "min       0.000000     0.000000     2.000000     1.000000  \n",
       "25%       0.000000     0.000000    47.000000     9.000000  \n",
       "50%       0.000000     1.000000    94.000000    17.000000  \n",
       "75%       0.577350     1.000000   179.000000    32.000000  \n",
       "max       0.577350     1.000000  7050.000000   819.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_comments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ce3b86f848>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_comments['label'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labeled_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords,\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mluis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'f', 'h', 'i', 'j', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'u', 'v'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(comments).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mluis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "#Get POS tags for comments and save as a string\n",
    "comment_tags = []\n",
    "for t in comments:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    comment_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(comment_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 0 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.61      0.57       330\n",
      "           1       0.63      0.56      0.59       385\n",
      "\n",
      "    accuracy                           0.58       715\n",
      "   macro avg       0.58      0.59      0.58       715\n",
      "weighted avg       0.59      0.58      0.58       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 1 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       330\n",
      "           1       0.63      0.59      0.61       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.60       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 2 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.58       330\n",
      "           1       0.63      0.60      0.62       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.60       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 3 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       330\n",
      "           1       0.64      0.59      0.61       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.60       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 4 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       330\n",
      "           1       0.63      0.59      0.61       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.59       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 5 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       330\n",
      "           1       0.63      0.59      0.61       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.59       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 6 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       330\n",
      "           1       0.64      0.59      0.61       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.60       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 7 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.57       330\n",
      "           1       0.63      0.61      0.62       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.60       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 8 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.58       330\n",
      "           1       0.63      0.59      0.61       385\n",
      "\n",
      "    accuracy                           0.59       715\n",
      "   macro avg       0.59      0.59      0.59       715\n",
      "weighted avg       0.60      0.59      0.59       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- 9 --------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.59       330\n",
      "           1       0.64      0.59      0.61       385\n",
      "\n",
      "    accuracy                           0.60       715\n",
      "   macro avg       0.60      0.60      0.60       715\n",
      "weighted avg       0.60      0.60      0.60       715\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.60       330\n",
      "           1       0.65      0.66      0.66       385\n",
      "\n",
      "    accuracy                           0.63       715\n",
      "   macro avg       0.63      0.63      0.63       715\n",
      "weighted avg       0.63      0.63      0.63       715\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Arquivos e Programas\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "X = pd.DataFrame(M)\n",
    "y = df['label'].astype(int)\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model_svc = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_train, y_train)\n",
    "    model_lr = LogisticRegression(class_weight='balanced',penalty='l2',C=0.01).fit(X_train, y_train)\n",
    "\n",
    "    y_preds_svc = model_svc.predict(X_test)\n",
    "    y_preds_lr = model_lr.predict(X_test)\n",
    "    print(f'------------------- {i} --------------------')\n",
    "    print('SVM')\n",
    "    print(classification_report(y_test,y_preds_svc))\n",
    "    print('LR')\n",
    "    print(classification_report(y_test,y_preds_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/baseline_results.txt') as fp:\n",
    "    file_lines = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = {\n",
    "    'precision_0': [],\n",
    "    'recall_0': [],\n",
    "    'f1_0': [],\n",
    "    'precision_1': [],\n",
    "    'recall_1': [],\n",
    "    'f1_1': [],\n",
    "    'f1_macro': []\n",
    "}\n",
    "\n",
    "lr = {\n",
    "    'precision_0': [],\n",
    "    'recall_0': [],\n",
    "    'f1_0': [],\n",
    "    'precision_1': [],\n",
    "    'recall_1': [],\n",
    "    'f1_1': [],\n",
    "    'f1_macro': []\n",
    "}\n",
    "\n",
    "data = []\n",
    "for line in file_lines:\n",
    "    is_data_line = ('----' not in line) and\\\n",
    "                   ('SVM' not in line) and\\\n",
    "                   ('LR' not in line) and\\\n",
    "                   ('precision' not in line) and\\\n",
    "                   ('\\n' != line) and \\\n",
    "                   ('accuracy' not in line) and\\\n",
    "                   ('weightedavg' not in line)\n",
    "    if is_data_line:\n",
    "        data.append(line.replace('           ','')\\\n",
    "                        .replace('      ', ';')\\\n",
    "                        .replace(' ', '')\\\n",
    "                        .replace('\\n', '')\\\n",
    "                        .split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data),8):\n",
    "    row = data[i:i+8]\n",
    "    svm['precision_0'].append(float(row[0][1]))\n",
    "    svm['recall_0'].append(float(row[0][2]))\n",
    "    svm['f1_0'].append(float(row[0][3]))\n",
    "    svm['precision_1'].append(float(row[1][1]))\n",
    "    svm['recall_1'].append(float(row[1][2]))\n",
    "    svm['f1_1'].append(float(row[1][3]))\n",
    "    svm['f1_macro'].append(float(row[2][3]))\n",
    "    lr['f1_macro'].append(float(row[6][3]))\n",
    "    lr['precision_0'].append(float(row[4][1]))\n",
    "    lr['recall_0'].append(float(row[4][2]))\n",
    "    lr['f1_0'].append(float(row[4][3]))\n",
    "    lr['precision_1'].append(float(row[5][1]))\n",
    "    lr['recall_1'].append(float(row[5][2]))\n",
    "    lr['f1_1'].append(float(row[5][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
